NAME: Devyan Biswas
EMAIL: devyanbiswas@outlook.com
ID: 804988161

--------------------------------------------------------------------------------------------------------
DESCRIPTION
--------------------------------------------------------------------------------------------------------
In general, add uses a protected add function with different options to showcase race conditions and locking mechanisms. Part 2 is along similar lines, by representing race conditions for complex data structures. SortedList.h and .c are what this is implemented by, and list is what maintains threads and performs operations on doubly linked list.

In this part, we essentially allow for the overall linked list to be partitioned into multiple lists, passed in by user, to reduce contention for the resource between threads.

Contents:
.png files of the different tests done by testb.sh, and graphed by lab2b.gp
lab2b.gp: gnuplot code for graphing the different list runs as defined by the spec
SortedList.h and .c: implementations of the SortedList doubly linked list used in lab2_list.c
lab2b_list.csv: is the result of testb.sh tests, used by lab2b.gp for data
profile.out: profiles CPU usage of the function pass_to_thread, which is the funtion to...pass...to thread...at creation
Makefile with various testings:
	 default ... the lab2_list executable (compiling with the -Wall and -Wextra options).
	 tests ... run all specified test cases to generate CSV results
         prfile ... run tests with profiling tools to generate an execution profiling report
       	 graphs ... use gnuplot to generate the required graphs
         dist ... create the deliverable tarball
         clean ... delete all programs and output generated by the Makefile

--------------------------------------------------------------------------------------------------------
QUESTIONS
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
QUESTION 2.3.1 - CPU time in the basic list implementation:
- Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?
- Why do you believe these to be the most expensive parts of the code?
- Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
- Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

Answers:
- For part a, we are testing add and list. For add, the lock is probably the same, since the operation is the same for all. In the list.
the list options would take up most of the time, I believe.
- The reason is becasue there is no other potential source of overhead; since we have 1-2 threads, contention for the resource isnt really a problem, so the operations themselves would be
the most reasonable source of CPU time.
- In the add, the time is most definetely spent in the spin-locks. Same for list.
- For add, the time is probably spent in the context switches. For list, the timing might be comparatively low(timing for the context switch) if there are a lot
of threads; in that case, the list operations again would be the main source of CPU time. Otherwise, the timing for list is based on context switch.
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
QUESTION 2.3.2 - Execution Profiling:
- Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?
- Why does this operation become so expensive with large numbers of threads?

Answers:
- Based on profile.out, it seems the spin lock waits consume a massive amount of time comparitively speaking. 
- This makes sense, due to the nature of spin locks, as threads increase, there is more risk for contention of the lock, and so longer(apparently so) wait times.
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
QUESTION 2.3.3 - Mutex Wait Time:
- Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
- Why does the average lock-wait time rise so dramatically with the number of contending threads?
- Why does the completion time per operation rise (less dramatically) with the number of contending threads?
- How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

Answers:
- With mroe threads wanting a lock, the overall time waiting for locks across all threads increases.
- The rise makes sense. The reason it happens less dramatically is because average lock-wait time is based on the wait time for the lock over 
the time necessary to get to that point, ie the context switch. This is what it takes into account.
- The wait time is the wall time for each thread. Since there can be overlap in waiting b/t threads, this causes the faster increase.
--------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------
QUESTION 2.3.4 - Performance of Partitioned Lists
- Explain the change in performance of the synchronized methods as a function of the number of lists.
- Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
- It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

Answers:
- For synced methods, higher number of lists means a shorter overall list length for each thread's perspective, which means a lower chance of lock contention.
- Theoretically speaking, you are only limited by number of threads. There can be a point where, based on the overall size, number of lists, and threads, you can create a 
throughput with contention for lock with probability 0.
- Not really, since smaller lists means smaller critical section, which means lower probability of contention.
--------------------------------------------------------------------------------------------------------

